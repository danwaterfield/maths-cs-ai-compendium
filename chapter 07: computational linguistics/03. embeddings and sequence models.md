# Embeddings and Sequence Models

- Word embeddings: Word2Vec (CBOW, skip-gram), GloVe, FastText
- Embedding evaluation: analogy tasks, similarity benchmarks
- RNNs: vanilla RNN, backpropagation through time, vanishing/exploding gradients
- LSTMs: forget gate, input gate, output gate, cell state
- GRUs: reset and update gates
- Bidirectional RNNs, deep stacked RNNs
- Sequence-to-sequence architecture, encoder-decoder framework
- Attention mechanisms: Bahdanau (additive), Luong (multiplicative)
- Beam search decoding
- 1D CNNs for text: TextCNN, dilated causal convolutions
- Contextual embeddings: ELMo