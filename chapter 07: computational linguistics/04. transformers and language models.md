# Transformers and Language Models

- Self-attention: scaled dot-product attention, multi-head attention
- Positional encoding: sinusoidal, learned, RoPE, ALiBi
- Transformer block: layer norm, residual connections, feed-forward sublayer
- Encoder-only models: BERT, masked language modelling, NSP
- Decoder-only models: GPT family, causal language modelling, autoregressive generation
- Encoder-decoder models: T5, BART, span corruption, denoising objectives
- Fine-tuning strategies: full fine-tuning, adapters, LoRA, prefix tuning
- Prompt engineering and in-context learning
- Scaling laws: Kaplan, Chinchilla, compute-optimal training
- Mixture of Experts (MoE): gating functions, top-k routing, load balancing losses, expert parallelism